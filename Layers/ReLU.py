class ReLU:
    def __init__(self):
        pass

    def forward(self,input_tensor):
        pass
    # returns a tensor that serves as the input_tensor for the next layer

    def backward(self,error_tensor):
        pass
    # returns a tensor that serves as the error_tensor for the previous layer.

# test your implementation using cmdline param TestReLU

